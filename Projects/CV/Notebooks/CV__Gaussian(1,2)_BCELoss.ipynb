{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwcy09TbgDCg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as f\n",
        "import matplotlib as plt\n",
        "import torch.optim as optim # for our shared adam class optimizer which will have the BCE loss term and also the KL divergence\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "bs = 100\n",
        "# MNIST Dataset\n",
        "train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
        "test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n",
        "\n",
        "# Data Loader (Input Pipeline)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPmB-s3Igsj4"
      },
      "outputs": [],
      "source": [
        "#\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eoW_Hkd8hzbc"
      },
      "outputs": [],
      "source": [
        "class VAE(nn.Module):\n",
        "  def __init__(self, x_dim,h_dim1, h_dim2, z_dim):\n",
        "    super(VAE, self).__init__()\n",
        "\n",
        "    #creating the required NNS for the ENCODER part\n",
        "    self.fc1=nn.Linear(x_dim,h_dim1)\n",
        "    self.fc2=nn.Linear(h_dim1,h_dim2)\n",
        "    self.fc01=nn.Linear(h_dim2,z_dim) # for the mean\n",
        "    self.fc02=nn.Linear(h_dim2,z_dim) # for the varianve\n",
        "\n",
        "    #now for he decoder part'''\n",
        "    self.fc3=nn.Linear(z_dim,h_dim2)\n",
        "    self.fc4=nn.Linear(h_dim2,h_dim1)\n",
        "    self.fc5=nn.Linear(h_dim1,x_dim)\n",
        "\n",
        "  def encoder(self,x):\n",
        "    h=f.relu(self.fc1(x))\n",
        "    h=f.relu(self.fc2(h))\n",
        "    m=self.fc01(h) # computed the mean tensor\n",
        "    v=self.fc02(h) # computed the log of the std dev\n",
        "    return m, v\n",
        "\n",
        "  def sampling(self,m,v):\n",
        "    std = torch.exp(0.5*v)\n",
        "    eps = torch.randn_like(std)\n",
        "    return eps.mul(std).add(m) # returns the sampled space from the available latent space\n",
        "\n",
        "  def decoder(self,z):\n",
        "    h=f.relu(self.fc3(z))\n",
        "    h=f.relu(self.fc4(h))\n",
        "    return f.sigmoid(self.fc5(h)) # here we could also experiment with the tanh function int=stead of sigmoid\n",
        "\n",
        "  def forward(self,x):\n",
        "    m, lv= self.encoder(x.view(-1,784))\n",
        "    z= self.sampling(m,lv)\n",
        "    return self.decoder(z), m, lv\n",
        "\n",
        "\n",
        "#building an instance of the class or called the model xD\n",
        "vae=VAE(x_dim=784, h_dim1=512,h_dim2= 256, z_dim=2)\n",
        "\n",
        "#setting up the device diagnostic code here also\n",
        "if torch.cuda.is_available():\n",
        "  vae.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkChQHUS2xWf"
      },
      "outputs": [],
      "source": [
        "vae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYVYmgJLnN8s"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(vae.parameters(), lr=0.01)\n",
        "\n",
        "def loss_function(recon_x, x, mu, log_var):\n",
        "    BCE = f.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    #MSE = f.mse_loss(recon_x, x.view(-1, 784), reduction='sum')\n",
        "    KLD = -0.5 * torch.sum(1 + log_var-2*(np.log(2)) - ((log_var.exp()+((mu-1).pow(2)))/2)  )\n",
        "    return BCE + KLD\n",
        "    #return MSE + KLD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN5ZJ6Bx3agk"
      },
      "outputs": [],
      "source": [
        "def train(epoch):\n",
        "    vae.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, _) in enumerate(train_loader):\n",
        "        data = data.cuda()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        recon_batch, mu, log_var = vae(data)\n",
        "        loss = loss_function(recon_batch, data, mu, log_var)\n",
        "\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item() / len(data)))\n",
        "    print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPskqDbm4GTC"
      },
      "outputs": [],
      "source": [
        "def test():\n",
        "    vae.eval()\n",
        "    test_loss= 0\n",
        "    with torch.no_grad():\n",
        "        for data, _ in test_loader:\n",
        "            data = data.cuda()\n",
        "            recon, mu, log_var = vae(data)\n",
        "\n",
        "            # sum up batch loss\n",
        "            test_loss += loss_function(recon, data, mu, log_var).item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}'.format(test_loss))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3aBcwzU4KBz"
      },
      "outputs": [],
      "source": [
        "for epoch in range(1, 51):\n",
        "    train(epoch)\n",
        "    test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    z = torch.randn(64, 2).cuda()\n",
        "    sample = vae.decoder(z).cuda()\n",
        "\n",
        "    save_image(sample.view(64, 1, 28, 28), './samples/sample_' + '.png')"
      ],
      "metadata": {
        "id": "k1HjuZXYBnq_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}